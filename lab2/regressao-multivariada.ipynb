{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regressão Linear Multivariada com NumPy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>\n",
    "<b>Author:</b> Caio Batista</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import statsmodels.formula.api as sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Versão Vetorizada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$MSE(\\hat{w})=\\frac{1}{N}(y-\\hat{\\mathbf{w}}^T\\mathbf{x})^T(y-\\hat{\\mathbf{w}}^T\\mathbf{x})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seleção de features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das dimensões que se encontram disponíveis para criarmos o modelo, foram escolhidas as disciplinas de <b>Cálculo 1</b> e <b>Introdução a Computação</b> que combinadas obtiveram o menor erro quadrado. Elas respectivamente estão descritos no [arquivo csv](https://canvas.instructure.com/courses/1198332/files/55584499/download?verifier=UvJGkZFGGe7WmktSFeBWe7zsZT8LOVvGW6FzRfKl) como <b>Cálculo1</b> e <b>IC</b>.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_mse_vectorized(w,X,Y):\n",
    "    res = Y - np.dot(X,w)\n",
    "    totalError = np.dot(res.T,res)\n",
    "    return totalError / float(len(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def step_gradient_vectorized(w_current,X,Y,learningRate):\n",
    "    res = Y - np.dot(X,w_current)\n",
    "    b_gradient = np.sum(res)\n",
    "    X = X[:,1][:,np.newaxis]\n",
    "    m_gradient = np.sum(np.multiply(res,X))\n",
    "    new_w = np.array([(w_current[0] + (2 * learningRate * b_gradient)),\n",
    "             (w_current[1] + (2 * learningRate * m_gradient))])\n",
    "    return [new_w,b_gradient,m_gradient]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gradient_descent_runner_vectorized(starting_w, X,Y, learning_rate, epsilon):\n",
    "    w = starting_w\n",
    "    grad = np.array([np.inf,np.inf])\n",
    "    i = 0\n",
    "    while (np.linalg.norm(grad)>=epsilon):\n",
    "        w,b_gradient,m_gradient = step_gradient_vectorized(w, X, Y, learning_rate)\n",
    "        grad = np.array([b_gradient,m_gradient])\n",
    "        #print(np.linalg.norm(grad))\n",
    "        if i % 1000 == 0:\n",
    "            print(\"MSE na iteração {0} é de {1}\".format(i,compute_mse_vectorized(w, X, Y)))\n",
    "        i+= 1\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting gradient descent at w0 = [ 0.], w1 = [ 0.], error = [[ 54.47995386]]\n",
      "Running...\n",
      "MSE na iteração 0 é de [[ 5.89558809]]\n",
      "MSE na iteração 1000 é de [[ 0.5761655]]\n",
      "MSE na iteração 2000 é de [[ 0.68977175]]\n",
      "Gradiente descendente convergiu com w0 = [ 0.54025384], w1 = [ 0.42072107], error = [[ 0.69817919]]\n",
      "Versão vetorizada rodou em: 140.18630981445312 ms\n"
     ]
    }
   ],
   "source": [
    "points = np.genfromtxt(\"sample_treino.csv\", delimiter=\",\")\n",
    "points = np.c_[np.ones(len(points)),points]\n",
    "X = points[1:, [1,4]]\n",
    "Y = points[1:,-1][:,np.newaxis]\n",
    "init_w = np.zeros((2,1))\n",
    "learning_rate = 0.0001\n",
    "#num_iterations = 10000\n",
    "epsilon = 0.5\n",
    "print(\"Starting gradient descent at w0 = {0}, w1 = {1}, error = {2}\".format(init_w[0], init_w[1], compute_mse_vectorized(init_w, X,Y)))\n",
    "print(\"Running...\")\n",
    "tic = time.time()\n",
    "w = gradient_descent_runner_vectorized(init_w, X,Y, learning_rate, epsilon)\n",
    "toc = time.time()\n",
    "print(\"Gradiente descendente convergiu com w0 = {0}, w1 = {1}, error = {2}\".format(w[0], w[1], compute_mse_vectorized(w,X,Y)))\n",
    "print(\"Versão vetorizada rodou em: \" + str(1000*(toc-tic)) + \" ms\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cálculo usando o pacote \"sklearn\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizando a lib <b>sklearn</b> de Python, podemos criar um modelo linear baseado em scikit-learn. Como foi pedido no laboratório, aqui focaremos na comparação dos coeficientes feita entre os modelos da sessão anterio e dessa sessão. O objetivo aqui será avaliar qual desses modelos tem o menor erro para o caso de predição de notas para o CRA. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seleção de features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A seleção de features nesse caso foi a mesma que na sessão anterior, pois como queremos comparar os coeficientes nos mesmos cenários, apenas alterando o modelo linear, o restante das modificações se manteve a mesma."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Divisão entre treino e teste"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como a divisão de treino e teste era necessária para podermos verificar a acurácia do modelo, a divisão foi feita que 80% dos dados foram para treino e 20% para teste. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Código e plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients:  [ 0.05853151  0.51561943]\n",
      "Mean squared error: 0.35\n",
      "Variance score: 0.52\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n3M3MEuRwNzNLkMPdzCxBDnczswT9P+Qpkz0TKj7KAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7faeb6243198>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "calc1 = points[1:,1]\n",
    "lpt = points[1:,2]\n",
    "p1 = points[1:,3]\n",
    "ic = points[1:,4]\n",
    "calc2 = points[1:,5]\n",
    "cra = points[1:,6]\n",
    "\n",
    "a = np.array([calc1,ic])\n",
    "a = a.transpose()\n",
    "a_train = a[:-20]\n",
    "a_test = a[-20:]\n",
    "\n",
    "b = cra\n",
    "\n",
    "b_train = b[:-20]\n",
    "b_test = b[-20:]\n",
    "\n",
    "clf = linear_model.LinearRegression()\n",
    "clf.fit(a_train, b_train)\n",
    "\n",
    "cra_pred = clf.predict(a_test)\n",
    "\n",
    "print('Coefficients: ', clf.coef_)\n",
    "\n",
    "print(\"Mean squared error: %.2f\"\n",
    "      % mean_squared_error(b_test, cra_pred))\n",
    "# Explained variance score: 1 is perfect prediction\n",
    "print('Variance score: %.2f' % r2_score(b_test, cra_pred))\n",
    "\n",
    "calc1_plot = a_test[:,0]\n",
    "ic_plot =  a_test[:,1]\n",
    "# Plot outputs\n",
    "plt.scatter(calc1_plot, b_test,  color='black')\n",
    "plt.scatter(ic_plot, b_test,  color='red')\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusão dos resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Após ambos algoritmos serem executados podemos ver que o erro para o scikit-learn foi quase <b>duas vezes menor</b> que o do gradiente descendente. A explicação para isso pode se dar por uma escolha de learning rate equivocada. Já para os valores dos coeficientes não podemos ver nenhuma diferença expressiva para w1, os valores são bem próximos, diferentemente para w0 que o valor chega a ser quase 10 vezes maior. O número de iterações para o gradiente descentente se manteve em 2000."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explicação plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O plot no final dessa sessão descreve o comportamento das duas features em relação ao cra, em forma de gráfico de dispersão."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
